stage_1:
  target: stage1.RAE
  params:
    encoder_cls: 'Dinov3withNorm'
    encoder_config_path: 'facebook/dinov3-vitl16-pretrain-lvd1689m'
    encoder_input_size: 224
    encoder_params:
      dinov3_path: 'facebook/dinov3-vitl16-pretrain-lvd1689m'
      normalize: true
    decoder_config_path: 'configs/decoder/ViTXL'
    pretrained_decoder_path: 'models/decoders/dinov3/vitl16/ViTXL_n08/model.pt'
    noise_tau: 0.0
    reshape_to_2d: true
    normalization_stat_path: 'models/stats/dinov3/vitl16/imagenet1k/stat.pt'

stage_2:
  target: stage2.models.DDT.DiTwDDTHead
  params:
    input_size: 14  # 196 tokens = 14x14
    patch_size: 1
    in_channels: 1024  # DINOv3-L hidden size
    hidden_size: [1152, 2048]
    depth: [28, 2]
    num_heads: [16, 16]
    mlp_ratio: 4.0
    class_dropout_prob: 0.1
    num_classes: 1000
    use_qknorm: false
    use_swiglu: true
    use_rope: true
    use_rmsnorm: true
    wo_shift: false
    use_pos_embed: true

transport:
  params:
    path_type: 'Linear'
    prediction: 'velocity'
    loss_weight: null
    time_dist_type: 'uniform'

sampler:
  mode: ODE
  params:
    sampling_method: 'euler'
    num_steps: 50
    atol: 1.0e-6
    rtol: 1.0e-3
    reverse: false

guidance:
  method: 'cfg'
  scale: 1.0
  t_min: 0.0
  t_max: 1.0

misc:
  latent_size: [1024, 14, 14]  # [C, H, W] - 1024 channels, 14x14 spatial
  num_classes: 1000
  time_dist_shift_dim: 200704  # 14*14*1024
  time_dist_shift_base: 4096

training:
  global_seed: 0
  epochs: 1400
  global_batch_size: 1024
  grad_accum_steps: 1
  ema_decay: 0.9995
  num_workers: 4
  log_every: 100
  ckpt_every: 5000
  sample_every: 10000
  base_lr: 0.0002
  final_lr: 0.00002
  beta: [0.9, 0.95]
  wd: 0.0
  schedule_type: 'linear'
  decay_start_epoch: 40
  decay_end_epoch: 800
  clip_grad: 1.0